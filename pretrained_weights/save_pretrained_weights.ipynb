{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchtext\n",
    "\n",
    "os.chdir(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))\n",
    "\n",
    "from models.seq2seq import Seq2seq\n",
    "from loss.loss import Perplexity\n",
    "from evaluator.evaluator import Evaluator\n",
    "from dataset import fields\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = 'info'\n",
    "LOG_FORMAT = '%(asctime)s %(levelname)-6s %(message)s'\n",
    "logging.basicConfig(format=LOG_FORMAT, level=getattr(logging, log_level.upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_lists = []\n",
    "data_name = \"copy\"\n",
    "dir_name = \"separator_Ctype4_60\"\n",
    "data_sort = \"\"\n",
    "rnn = \"lstm\"\n",
    "iterator = list(range(1,11,1))\n",
    "\n",
    "data_path = \"data/\"+data_name+\"_rand/correction_\"+dir_name\n",
    "train_path = data_path+\"/data_train.txt\"\n",
    "config_path = \"models/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN is lstm\n",
      "data path: data/copy_rand/correction_separator_Ctype4_60\n",
      "src vocab size = 7\n",
      "tat vacab size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/venv/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"max_len\": 65,\n",
      "    \"embedding_size\": 20,\n",
      "    \"hidden_size\": 100,\n",
      "    \"input_dropout_p\": 0,\n",
      "    \"dropout_p\": 0,\n",
      "    \"n_layers\": 1,\n",
      "    \"bidirectional\": false,\n",
      "    \"rnn_cell\": \"lstm\",\n",
      "    \"variable_lengths\": true,\n",
      "    \"embedding\": null,\n",
      "    \"update_embedding\": true,\n",
      "    \"get_context_vector\": false,\n",
      "    \"use_attention\": true,\n",
      "    \"attn_layers\": 1,\n",
      "    \"hard_attn\": false,\n",
      "    \"position_embedding\": \"length\",\n",
      "    \"pos_add\": \"add\",\n",
      "    \"use_memory\": \"queue\",\n",
      "    \"memory_dim\": 5\n",
      "}\n",
      "pos_embedding.weight\n",
      "encoder.embedding.weight\n",
      "encoder.rnn.weight_ih_l0\n",
      "encoder.rnn.weight_hh_l0\n",
      "encoder.rnn.bias_ih_l0\n",
      "encoder.rnn.bias_hh_l0\n",
      "encoder.pos_embedding.weight\n",
      "encoder.W_n.weight\n",
      "encoder.W_n.bias\n",
      "encoder.W_a.weight\n",
      "encoder.W_a.bias\n",
      "encoder.W_sh.weight\n",
      "encoder.W_sh.bias\n",
      "decoder.embedding.weight\n",
      "decoder.pos_embedding.weight\n",
      "decoder.rnn.weight_ih_l0\n",
      "decoder.rnn.weight_hh_l0\n",
      "decoder.rnn.bias_ih_l0\n",
      "decoder.rnn.bias_hh_l0\n",
      "decoder.attention1.linear_out.weight\n",
      "decoder.attention1.linear_out.bias\n",
      "decoder.out.weight\n",
      "decoder.out.bias\n",
      "decoder.W_n.weight\n",
      "decoder.W_n.bias\n",
      "decoder.W_a.weight\n",
      "decoder.W_a.bias\n",
      "decoder.W_sh.weight\n",
      "decoder.W_sh.bias\n",
      "{\n",
      "    \"max_len\": 65,\n",
      "    \"embedding_size\": 20,\n",
      "    \"hidden_size\": 100,\n",
      "    \"input_dropout_p\": 0,\n",
      "    \"dropout_p\": 0,\n",
      "    \"n_layers\": 1,\n",
      "    \"bidirectional\": false,\n",
      "    \"rnn_cell\": \"lstm\",\n",
      "    \"variable_lengths\": true,\n",
      "    \"embedding\": null,\n",
      "    \"update_embedding\": true,\n",
      "    \"get_context_vector\": false,\n",
      "    \"use_attention\": true,\n",
      "    \"attn_layers\": 1,\n",
      "    \"hard_attn\": false,\n",
      "    \"position_embedding\": \"length\",\n",
      "    \"pos_add\": \"add\",\n",
      "    \"use_memory\": \"queue\",\n",
      "    \"memory_dim\": 5\n",
      "}\n",
      "pos_embedding.weight\n",
      "encoder.embedding.weight\n",
      "encoder.rnn.weight_ih_l0\n",
      "encoder.rnn.weight_hh_l0\n",
      "encoder.rnn.bias_ih_l0\n",
      "encoder.rnn.bias_hh_l0\n",
      "encoder.pos_embedding.weight\n",
      "encoder.W_n.weight\n",
      "encoder.W_n.bias\n",
      "encoder.W_a.weight\n",
      "encoder.W_a.bias\n",
      "encoder.W_sh.weight\n",
      "encoder.W_sh.bias\n",
      "decoder.embedding.weight\n",
      "decoder.pos_embedding.weight\n",
      "decoder.rnn.weight_ih_l0\n",
      "decoder.rnn.weight_hh_l0\n",
      "decoder.rnn.bias_ih_l0\n",
      "decoder.rnn.bias_hh_l0\n",
      "decoder.attention1.linear_out.weight\n",
      "decoder.attention1.linear_out.bias\n",
      "decoder.out.weight\n",
      "decoder.out.bias\n",
      "decoder.W_n.weight\n",
      "decoder.W_n.bias\n",
      "decoder.W_a.weight\n",
      "decoder.W_a.bias\n",
      "decoder.W_sh.weight\n",
      "decoder.W_sh.bias\n",
      "{\n",
      "    \"max_len\": 65,\n",
      "    \"embedding_size\": 20,\n",
      "    \"hidden_size\": 100,\n",
      "    \"input_dropout_p\": 0,\n",
      "    \"dropout_p\": 0,\n",
      "    \"n_layers\": 1,\n",
      "    \"bidirectional\": false,\n",
      "    \"rnn_cell\": \"lstm\",\n",
      "    \"variable_lengths\": true,\n",
      "    \"embedding\": null,\n",
      "    \"update_embedding\": true,\n",
      "    \"get_context_vector\": false,\n",
      "    \"use_attention\": true,\n",
      "    \"attn_layers\": 1,\n",
      "    \"hard_attn\": false,\n",
      "    \"position_embedding\": \"length\",\n",
      "    \"pos_add\": \"add\",\n",
      "    \"use_memory\": \"queue\",\n",
      "    \"memory_dim\": 5\n",
      "}\n",
      "pos_embedding.weight\n",
      "encoder.embedding.weight\n",
      "encoder.rnn.weight_ih_l0\n",
      "encoder.rnn.weight_hh_l0\n",
      "encoder.rnn.bias_ih_l0\n",
      "encoder.rnn.bias_hh_l0\n",
      "encoder.pos_embedding.weight\n",
      "encoder.W_n.weight\n",
      "encoder.W_n.bias\n",
      "encoder.W_a.weight\n",
      "encoder.W_a.bias\n",
      "encoder.W_sh.weight\n",
      "encoder.W_sh.bias\n",
      "decoder.embedding.weight\n",
      "decoder.pos_embedding.weight\n",
      "decoder.rnn.weight_ih_l0\n",
      "decoder.rnn.weight_hh_l0\n",
      "decoder.rnn.bias_ih_l0\n",
      "decoder.rnn.bias_hh_l0\n",
      "decoder.attention1.linear_out.weight\n",
      "decoder.attention1.linear_out.bias\n",
      "decoder.out.weight\n",
      "decoder.out.bias\n",
      "decoder.W_n.weight\n",
      "decoder.W_n.bias\n",
      "decoder.W_a.weight\n",
      "decoder.W_a.bias\n",
      "decoder.W_sh.weight\n",
      "decoder.W_sh.bias\n",
      "{\n",
      "    \"max_len\": 65,\n",
      "    \"embedding_size\": 20,\n",
      "    \"hidden_size\": 100,\n",
      "    \"input_dropout_p\": 0,\n",
      "    \"dropout_p\": 0,\n",
      "    \"n_layers\": 1,\n",
      "    \"bidirectional\": false,\n",
      "    \"rnn_cell\": \"lstm\",\n",
      "    \"variable_lengths\": true,\n",
      "    \"embedding\": null,\n",
      "    \"update_embedding\": true,\n",
      "    \"get_context_vector\": false,\n",
      "    \"use_attention\": true,\n",
      "    \"attn_layers\": 1,\n",
      "    \"hard_attn\": false,\n",
      "    \"position_embedding\": \"length\",\n",
      "    \"pos_add\": \"add\",\n",
      "    \"use_memory\": \"queue\",\n",
      "    \"memory_dim\": 5\n",
      "}\n",
      "pos_embedding.weight\n",
      "encoder.embedding.weight\n",
      "encoder.rnn.weight_ih_l0\n",
      "encoder.rnn.weight_hh_l0\n",
      "encoder.rnn.bias_ih_l0\n",
      "encoder.rnn.bias_hh_l0\n",
      "encoder.pos_embedding.weight\n",
      "encoder.W_n.weight\n",
      "encoder.W_n.bias\n",
      "encoder.W_a.weight\n",
      "encoder.W_a.bias\n",
      "encoder.W_sh.weight\n",
      "encoder.W_sh.bias\n",
      "decoder.embedding.weight\n",
      "decoder.pos_embedding.weight\n",
      "decoder.rnn.weight_ih_l0\n",
      "decoder.rnn.weight_hh_l0\n",
      "decoder.rnn.bias_ih_l0\n",
      "decoder.rnn.bias_hh_l0\n",
      "decoder.attention1.linear_out.weight\n",
      "decoder.attention1.linear_out.bias\n",
      "decoder.out.weight\n",
      "decoder.out.bias\n",
      "decoder.W_n.weight\n",
      "decoder.W_n.bias\n",
      "decoder.W_a.weight\n",
      "decoder.W_a.bias\n",
      "decoder.W_sh.weight\n",
      "decoder.W_sh.bias\n",
      "{\n",
      "    \"max_len\": 65,\n",
      "    \"embedding_size\": 20,\n",
      "    \"hidden_size\": 100,\n",
      "    \"input_dropout_p\": 0,\n",
      "    \"dropout_p\": 0,\n",
      "    \"n_layers\": 1,\n",
      "    \"bidirectional\": false,\n",
      "    \"rnn_cell\": \"lstm\",\n",
      "    \"variable_lengths\": true,\n",
      "    \"embedding\": null,\n",
      "    \"update_embedding\": true,\n",
      "    \"get_context_vector\": false,\n",
      "    \"use_attention\": true,\n",
      "    \"attn_layers\": 1,\n",
      "    \"hard_attn\": false,\n",
      "    \"position_embedding\": \"length\",\n",
      "    \"pos_add\": \"add\",\n",
      "    \"use_memory\": \"queue\",\n",
      "    \"memory_dim\": 5\n",
      "}\n",
      "pos_embedding.weight\n",
      "encoder.embedding.weight\n",
      "encoder.rnn.weight_ih_l0\n",
      "encoder.rnn.weight_hh_l0\n",
      "encoder.rnn.bias_ih_l0\n",
      "encoder.rnn.bias_hh_l0\n",
      "encoder.pos_embedding.weight\n",
      "encoder.W_n.weight\n",
      "encoder.W_n.bias\n",
      "encoder.W_a.weight\n",
      "encoder.W_a.bias\n",
      "encoder.W_sh.weight\n",
      "encoder.W_sh.bias\n",
      "decoder.embedding.weight\n",
      "decoder.pos_embedding.weight\n",
      "decoder.rnn.weight_ih_l0\n",
      "decoder.rnn.weight_hh_l0\n",
      "decoder.rnn.bias_ih_l0\n",
      "decoder.rnn.bias_hh_l0\n",
      "decoder.attention1.linear_out.weight\n",
      "decoder.attention1.linear_out.bias\n",
      "decoder.out.weight\n",
      "decoder.out.bias\n",
      "decoder.W_n.weight\n",
      "decoder.W_n.bias\n",
      "decoder.W_a.weight\n",
      "decoder.W_a.bias\n",
      "decoder.W_sh.weight\n",
      "decoder.W_sh.bias\n",
      "{\n",
      "    \"max_len\": 65,\n",
      "    \"embedding_size\": 20,\n",
      "    \"hidden_size\": 100,\n",
      "    \"input_dropout_p\": 0,\n",
      "    \"dropout_p\": 0,\n",
      "    \"n_layers\": 1,\n",
      "    \"bidirectional\": false,\n",
      "    \"rnn_cell\": \"lstm\",\n",
      "    \"variable_lengths\": true,\n",
      "    \"embedding\": null,\n",
      "    \"update_embedding\": true,\n",
      "    \"get_context_vector\": false,\n",
      "    \"use_attention\": true,\n",
      "    \"attn_layers\": 1,\n",
      "    \"hard_attn\": false,\n",
      "    \"position_embedding\": \"length\",\n",
      "    \"pos_add\": \"add\",\n",
      "    \"use_memory\": \"queue\",\n",
      "    \"memory_dim\": 5\n",
      "}\n",
      "pos_embedding.weight\n",
      "encoder.embedding.weight\n",
      "encoder.rnn.weight_ih_l0\n",
      "encoder.rnn.weight_hh_l0\n",
      "encoder.rnn.bias_ih_l0\n",
      "encoder.rnn.bias_hh_l0\n",
      "encoder.pos_embedding.weight\n",
      "encoder.W_n.weight\n",
      "encoder.W_n.bias\n",
      "encoder.W_a.weight\n",
      "encoder.W_a.bias\n",
      "encoder.W_sh.weight\n",
      "encoder.W_sh.bias\n",
      "decoder.embedding.weight\n",
      "decoder.pos_embedding.weight\n",
      "decoder.rnn.weight_ih_l0\n",
      "decoder.rnn.weight_hh_l0\n",
      "decoder.rnn.bias_ih_l0\n",
      "decoder.rnn.bias_hh_l0\n",
      "decoder.attention1.linear_out.weight\n",
      "decoder.attention1.linear_out.bias\n",
      "decoder.out.weight\n",
      "decoder.out.bias\n",
      "decoder.W_n.weight\n",
      "decoder.W_n.bias\n",
      "decoder.W_a.weight\n",
      "decoder.W_a.bias\n",
      "decoder.W_sh.weight\n",
      "decoder.W_sh.bias\n",
      "{\n",
      "    \"max_len\": 65,\n",
      "    \"embedding_size\": 20,\n",
      "    \"hidden_size\": 100,\n",
      "    \"input_dropout_p\": 0,\n",
      "    \"dropout_p\": 0,\n",
      "    \"n_layers\": 1,\n",
      "    \"bidirectional\": false,\n",
      "    \"rnn_cell\": \"lstm\",\n",
      "    \"variable_lengths\": true,\n",
      "    \"embedding\": null,\n",
      "    \"update_embedding\": true,\n",
      "    \"get_context_vector\": false,\n",
      "    \"use_attention\": true,\n",
      "    \"attn_layers\": 1,\n",
      "    \"hard_attn\": false,\n",
      "    \"position_embedding\": \"length\",\n",
      "    \"pos_add\": \"add\",\n",
      "    \"use_memory\": \"queue\",\n",
      "    \"memory_dim\": 5\n",
      "}\n",
      "pos_embedding.weight\n",
      "encoder.embedding.weight\n",
      "encoder.rnn.weight_ih_l0\n",
      "encoder.rnn.weight_hh_l0\n",
      "encoder.rnn.bias_ih_l0\n",
      "encoder.rnn.bias_hh_l0\n",
      "encoder.pos_embedding.weight\n",
      "encoder.W_n.weight\n",
      "encoder.W_n.bias\n",
      "encoder.W_a.weight\n",
      "encoder.W_a.bias\n",
      "encoder.W_sh.weight\n",
      "encoder.W_sh.bias\n",
      "decoder.embedding.weight\n",
      "decoder.pos_embedding.weight\n",
      "decoder.rnn.weight_ih_l0\n",
      "decoder.rnn.weight_hh_l0\n",
      "decoder.rnn.bias_ih_l0\n",
      "decoder.rnn.bias_hh_l0\n",
      "decoder.attention1.linear_out.weight\n",
      "decoder.attention1.linear_out.bias\n",
      "decoder.out.weight\n",
      "decoder.out.bias\n",
      "decoder.W_n.weight\n",
      "decoder.W_n.bias\n",
      "decoder.W_a.weight\n",
      "decoder.W_a.bias\n",
      "decoder.W_sh.weight\n",
      "decoder.W_sh.bias\n",
      "{\n",
      "    \"max_len\": 65,\n",
      "    \"embedding_size\": 20,\n",
      "    \"hidden_size\": 100,\n",
      "    \"input_dropout_p\": 0,\n",
      "    \"dropout_p\": 0,\n",
      "    \"n_layers\": 1,\n",
      "    \"bidirectional\": false,\n",
      "    \"rnn_cell\": \"lstm\",\n",
      "    \"variable_lengths\": true,\n",
      "    \"embedding\": null,\n",
      "    \"update_embedding\": true,\n",
      "    \"get_context_vector\": false,\n",
      "    \"use_attention\": true,\n",
      "    \"attn_layers\": 1,\n",
      "    \"hard_attn\": false,\n",
      "    \"position_embedding\": \"length\",\n",
      "    \"pos_add\": \"add\",\n",
      "    \"use_memory\": \"queue\",\n",
      "    \"memory_dim\": 5\n",
      "}\n",
      "pos_embedding.weight\n",
      "encoder.embedding.weight\n",
      "encoder.rnn.weight_ih_l0\n",
      "encoder.rnn.weight_hh_l0\n",
      "encoder.rnn.bias_ih_l0\n",
      "encoder.rnn.bias_hh_l0\n",
      "encoder.pos_embedding.weight\n",
      "encoder.W_n.weight\n",
      "encoder.W_n.bias\n",
      "encoder.W_a.weight\n",
      "encoder.W_a.bias\n",
      "encoder.W_sh.weight\n",
      "encoder.W_sh.bias\n",
      "decoder.embedding.weight\n",
      "decoder.pos_embedding.weight\n",
      "decoder.rnn.weight_ih_l0\n",
      "decoder.rnn.weight_hh_l0\n",
      "decoder.rnn.bias_ih_l0\n",
      "decoder.rnn.bias_hh_l0\n",
      "decoder.attention1.linear_out.weight\n",
      "decoder.attention1.linear_out.bias\n",
      "decoder.out.weight\n",
      "decoder.out.bias\n",
      "decoder.W_n.weight\n",
      "decoder.W_n.bias\n",
      "decoder.W_a.weight\n",
      "decoder.W_a.bias\n",
      "decoder.W_sh.weight\n",
      "decoder.W_sh.bias\n",
      "{\n",
      "    \"max_len\": 65,\n",
      "    \"embedding_size\": 20,\n",
      "    \"hidden_size\": 100,\n",
      "    \"input_dropout_p\": 0,\n",
      "    \"dropout_p\": 0,\n",
      "    \"n_layers\": 1,\n",
      "    \"bidirectional\": false,\n",
      "    \"rnn_cell\": \"lstm\",\n",
      "    \"variable_lengths\": true,\n",
      "    \"embedding\": null,\n",
      "    \"update_embedding\": true,\n",
      "    \"get_context_vector\": false,\n",
      "    \"use_attention\": true,\n",
      "    \"attn_layers\": 1,\n",
      "    \"hard_attn\": false,\n",
      "    \"position_embedding\": \"length\",\n",
      "    \"pos_add\": \"add\",\n",
      "    \"use_memory\": \"queue\",\n",
      "    \"memory_dim\": 5\n",
      "}\n",
      "pos_embedding.weight\n",
      "encoder.embedding.weight\n",
      "encoder.rnn.weight_ih_l0\n",
      "encoder.rnn.weight_hh_l0\n",
      "encoder.rnn.bias_ih_l0\n",
      "encoder.rnn.bias_hh_l0\n",
      "encoder.pos_embedding.weight\n",
      "encoder.W_n.weight\n",
      "encoder.W_n.bias\n",
      "encoder.W_a.weight\n",
      "encoder.W_a.bias\n",
      "encoder.W_sh.weight\n",
      "encoder.W_sh.bias\n",
      "decoder.embedding.weight\n",
      "decoder.pos_embedding.weight\n",
      "decoder.rnn.weight_ih_l0\n",
      "decoder.rnn.weight_hh_l0\n",
      "decoder.rnn.bias_ih_l0\n",
      "decoder.rnn.bias_hh_l0\n",
      "decoder.attention1.linear_out.weight\n",
      "decoder.attention1.linear_out.bias\n",
      "decoder.out.weight\n",
      "decoder.out.bias\n",
      "decoder.W_n.weight\n",
      "decoder.W_n.bias\n",
      "decoder.W_a.weight\n",
      "decoder.W_a.bias\n",
      "decoder.W_sh.weight\n",
      "decoder.W_sh.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN is %s\" % rnn)\n",
    "print(\"data path: %s\" % data_path)\n",
    "f1_score_list = []\n",
    "\n",
    "# Prepare dataset\n",
    "max_len = 65\n",
    "src = fields.SourceField()\n",
    "srcp = fields.SourceField()\n",
    "tgt = fields.TargetField()\n",
    "tgtp = fields.TargetField()\n",
    "def len_filter(example):\n",
    "    return len(example.src) <= max_len and len(example.tgt) <= max_len\n",
    "train = torchtext.data.TabularDataset(\n",
    "    path=train_path, format='tsv',\n",
    "    fields=[('src', src), ('tgt', tgt)],\n",
    "    filter_pred=len_filter\n",
    ")\n",
    "src.build_vocab(train)\n",
    "tgt.build_vocab(train)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab\n",
    "\n",
    "print(\"src vocab size = %d\" % (len(src.vocab)))\n",
    "print(\"tat vacab size = %d\" % (len(tgt.vocab)))\n",
    "\n",
    "# Prepare loss\n",
    "weight = torch.ones(len(tgt.vocab))\n",
    "pad = tgt.vocab.stoi[tgt.pad_token]\n",
    "loss = Perplexity(weight, pad)\n",
    "if torch.cuda.is_available():\n",
    "    loss.cuda()\n",
    "\n",
    "# Model\n",
    "evaluator = Evaluator(loss=loss, batch_size=32)\n",
    "\n",
    "optimizer = \"Adam\"\n",
    "seq2seq = None\n",
    "config_json = open(config_path).read()\n",
    "config = json.loads(config_json)\n",
    "config[\"max_len\"] = max_len\n",
    "config[\"hidden_size\"] = 100\n",
    "config[\"rnn_cell\"] = rnn\n",
    "config[\"embedding_size\"] = 20\n",
    "config[\"use_attention\"] = True\n",
    "config[\"position_embedding\"] = \"length\"\n",
    "config[\"use_memory\"] = \"queue\"\n",
    "#config[\"pos_add\"] = \"cat\"\n",
    "\n",
    "\n",
    "for i in iterator:\n",
    "    save_path = (data_name + \"_rand_\" + dir_name\n",
    "                    + (\"_att\" if config[\"use_attention\"] else \"\")\n",
    "                    + (\"_with_pos_\" + config[\"position_embedding\"] if config[\"position_embedding\"] is not None else \"\")\n",
    "                    + (\"_cat\" if config[\"pos_add\"] == \"cat\" else \"\")\n",
    "                    + (\"_use_stack\" if config[\"use_memory\"] == \"stack\" else \"\")\n",
    "                    + (\"_use_queue\" if config[\"use_memory\"] == \"queue\" else \"\")\n",
    "                    + \"_emb\" + str(config[\"embedding_size\"])\n",
    "                    + \"_hidden\" + str(config[\"hidden_size\"])\n",
    "                    + \"_\"+rnn+\"_\"+str(i))\n",
    "\n",
    "    print(json.dumps(config, indent=4))\n",
    "    seq2seq = Seq2seq(config, len(src.vocab), len(tgt.vocab), tgt.sos_id, tgt.eos_id)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        seq2seq.cuda()\n",
    "\n",
    "    for param in seq2seq.parameters():\n",
    "        param.data.uniform_(-0.08, 0.08)\n",
    "\n",
    "    log_path = \"log/pth/\"+save_path+\"_model_save.pth\"\n",
    "    seq2seq.load_state_dict(torch.load(log_path))\n",
    "    for var_name in seq2seq.state_dict():\n",
    "        print(var_name)\n",
    "\n",
    "    encoder_pos_weight = seq2seq.state_dict()['encoder.pos_embedding.weight'].cpu().numpy()\n",
    "    decoder_pos_weight = seq2seq.state_dict()['decoder.pos_embedding.weight'].cpu().numpy()\n",
    "    \n",
    "    save_path = \"pretrained_weights/\" + save_path\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    np.save(save_path+\"/encoder_pos_weight.npy\", encoder_pos_weight)\n",
    "    np.save(save_path+\"/decoder_pos_weight.npy\", decoder_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
