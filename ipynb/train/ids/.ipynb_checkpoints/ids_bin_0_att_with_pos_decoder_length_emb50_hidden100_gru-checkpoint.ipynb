{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameter\n",
    "- Attention = True\n",
    "- Teacher Forcing Ratio = 0.5\n",
    "- Layer = 1\n",
    "- Batch size = 128\n",
    "- Learning rate = 0.001\n",
    "- Hidden unit = 200\n",
    "- Epochs = 100\n",
    "- N = 50\n",
    "- Data Length = 100K\n",
    "- Data = [single_Ctype4_error_rate_1]\n",
    "- Deduplication\n",
    "- Random split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "\n",
    "import useful packages for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchtext\n",
    "\n",
    "os.chdir(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))))))\n",
    "\n",
    "from models.trainer import Trainer\n",
    "from models.seq2seq import Seq2seq\n",
    "from loss.loss import Perplexity\n",
    "from dataset.load_data import LoadData\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = 'info'\n",
    "LOG_FORMAT = '%(asctime)s %(levelname)-6s %(message)s'\n",
    "logging.basicConfig(format=LOG_FORMAT, level=getattr(logging, log_level.upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_accuracy = []\n",
    "sentence_accuracy = []\n",
    "f1_score = []\n",
    "best_f1_score = []\n",
    "rnn = \"gru\"\n",
    "data_name = \"ids\"\n",
    "pretrained_dir_name = None\n",
    "iterator = list(range(1,11,1))\n",
    "epochs = 100\n",
    "\n",
    "data_path = \"data/network_inputs/iitk-\"+data_name+\"-1189/bin_\"\n",
    "config_path = \"models/config.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded shuffled data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/venv/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "2020-03-31 15:59:10,331 INFO   Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.002\n",
      "    weight_decay: 0\n",
      "), Scheduler: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"encoder_max_len\": 420,\n",
      "    \"decoder_max_len\": 40,\n",
      "    \"embedding_size\": 50,\n",
      "    \"hidden_size\": 100,\n",
      "    \"input_dropout_p\": 0,\n",
      "    \"dropout_p\": 0,\n",
      "    \"n_layers\": 1,\n",
      "    \"bidirectional\": false,\n",
      "    \"rnn_cell\": \"gru\",\n",
      "    \"variable_lengths\": true,\n",
      "    \"embedding\": null,\n",
      "    \"update_embedding\": true,\n",
      "    \"get_context_vector\": false,\n",
      "    \"use_attention\": true,\n",
      "    \"attn_layers\": 1,\n",
      "    \"hard_attn\": false,\n",
      "    \"encoder_position_embedding\": null,\n",
      "    \"decoder_position_embedding\": \"length\",\n",
      "    \"pos_add\": \"add\",\n",
      "    \"use_memory\": null,\n",
      "    \"memory_dim\": 5\n",
      "}\n",
      "Save_path : typo_bin_0_att_with_pos_decoder_length_emb50_hidden100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/venv/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "for i in iterator:\n",
    "    for bins in range(5):\n",
    "        dataset = LoadData(data_path + str(bins))\n",
    "        vocab = tl_dict = dataset.get_tl_dictionary()\n",
    "        vocab_size = dataset.vocabulary_size\n",
    "        sos_id = 1\n",
    "        eos_id = 1\n",
    "        pad = 0\n",
    "\n",
    "        weight = torch.ones(vocab_size)\n",
    "        loss = Perplexity(weight, pad)\n",
    "        if torch.cuda.is_available():\n",
    "            loss.cuda()\n",
    "\n",
    "        optimizer = \"Adam\"\n",
    "        seq2seq = None\n",
    "        config_json = open(config_path).read()\n",
    "        config = json.loads(config_json)\n",
    "        config[\"hidden_size\"] = 100\n",
    "        config[\"rnn_cell\"] = rnn\n",
    "        config[\"embedding_size\"] = 50\n",
    "        config[\"use_attention\"] = True\n",
    "        config[\"encoder_position_embedding\"] = None\n",
    "        config[\"decoder_position_embedding\"] = \"length\"\n",
    "        config[\"use_memory\"] = None\n",
    "        #config[\"pos_add\"] = \"cat\"\n",
    "\n",
    "        print(json.dumps(config, indent=4))\n",
    "\n",
    "        save_path = (data_name + \"_bin_\" + str(bins)\n",
    "                        + (\"_att\" if config[\"use_attention\"] else \"\")\n",
    "                        + (\"_with_pos\" if config[\"encoder_position_embedding\"] is not None \n",
    "                           or config[\"decoder_position_embedding\"] is not None else \"\")\n",
    "                        + (\"_encoder_\" + config[\"encoder_position_embedding\"]\n",
    "                           if config[\"encoder_position_embedding\"] is not None else \"\")\n",
    "                        + (\"_decoder_\" + config[\"decoder_position_embedding\"]\n",
    "                           if config[\"decoder_position_embedding\"] is not None else \"\")\n",
    "                        + (\"_cat\" if config[\"pos_add\"] == \"cat\" else \"\")\n",
    "                        + (\"_use_stack\" if config[\"use_memory\"] == \"stack\" else \"\")\n",
    "                        + (\"_use_queue\" if config[\"use_memory\"] == \"queue\" else \"\")\n",
    "                        + \"_emb\" + str(config[\"embedding_size\"])\n",
    "                        + \"_hidden\" + str(config[\"hidden_size\"])\n",
    "                        + (\"_pretrained\" if pretrained_dir_name is not None else \"\"))\n",
    "        print(\"Save_path : %s\" % save_path)\n",
    "        \n",
    "        if pretrained_dir_name is not None:\n",
    "            pretrained_path = (\"pretrained_weights/\"+ data_name + \"_\" + pretrained_dir_name\n",
    "                        + (\"_att\" if config[\"use_attention\"] else \"\")\n",
    "                        + (\"_with_pos\" if config[\"encoder_position_embedding\"] is not None \n",
    "                           or config[\"decoder_position_embedding\"] is not None else \"\")\n",
    "                        + (\"_encoder_\" + config[\"encoder_position_embedding\"]\n",
    "                           if config[\"encoder_position_embedding\"] is not None else \"\")\n",
    "                        + (\"_decoder_\" + config[\"decoder_position_embedding\"]\n",
    "                           if config[\"decoder_position_embedding\"] is not None else \"\")\n",
    "                        + (\"_cat\" if config[\"pos_add\"] == \"cat\" else \"\")\n",
    "                        + (\"_use_stack\" if config[\"use_memory\"] == \"stack\" else \"\")\n",
    "                        + (\"_use_queue\" if config[\"use_memory\"] == \"queue\" else \"\")\n",
    "                        + \"_emb\" + str(config[\"embedding_size\"])\n",
    "                        + \"_hidden\" + str(config[\"hidden_size\"])\n",
    "                        + \"_\"+rnn+\"_\"+str(i))\n",
    "            encoder_pos_weight = np.load(pretrained_path+\"/encoder_pos_weight.npy\")\n",
    "            decoder_pos_weight = np.load(pretrained_path+\"/decoder_pos_weight.npy\")\n",
    "            seq2seq = Seq2seq(config, vocab_size, vocab_size, sos_id, eos_id,\n",
    "                              encoder_pos_weight, decoder_pos_weight)\n",
    "        else :\n",
    "            seq2seq = Seq2seq(config, vocab_size, vocab_size, sos_id, eos_id)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            seq2seq.cuda()\n",
    "\n",
    "        for param in seq2seq.parameters():\n",
    "            param.data.uniform_(-0.08, 0.08)\n",
    "\n",
    "        # train\n",
    "        t = Trainer(loss=loss, batch_size=128,\n",
    "                    learning_rate=0.002,\n",
    "                    checkpoint_every=50,\n",
    "                    print_every=100,\n",
    "                    hidden_size=config[\"hidden_size\"],\n",
    "                    path=save_path,\n",
    "                    file_name=config[\"rnn_cell\"] + \"_\" + str(i))\n",
    "\n",
    "        seq2seq, ave_loss, character_accuracy_list, sentence_accuracy_list, f1_score_list = t.train(seq2seq, dataset,\n",
    "                                                                                 num_epochs=epochs,\n",
    "                                                                                 optimizer=optimizer,\n",
    "                                                                                 teacher_forcing_ratio=0.5)\n",
    "\n",
    "        character_accuracy.append(character_accuracy_list)\n",
    "        sentence_accuracy.append(sentence_accuracy_list)\n",
    "        f1_score.append(f1_score_list)\n",
    "        best_f1_score.append(max(f1_score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "for j in range(len(sentence_accuracy)):\n",
    "    plt.plot(list(range(1, len(sentence_accuracy[j])+1, 1))[::3], sentence_accuracy[j][::3], '-', LineWidth=3, label=str(j+1))\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title(data_name+\"_\"+rnn, fontsize=24)\n",
    "plt.xlabel('Epoch', fontsize=24)\n",
    "plt.ylabel('Sentence Accuracy', fontsize=24)\n",
    "plt.ylim([0, 1.02])\n",
    "plt.grid()\n",
    "#plt.savefig('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "for j in range(len(f1_score)):\n",
    "    plt.plot(list(range(1, len(f1_score[j])+1, 1))[::3], f1_score[j][::3], '-', LineWidth=3, label=str(j+1))\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title(data_name+\"_\"+rnn, fontsize=24)\n",
    "plt.xlabel('Epoch', fontsize=24)\n",
    "plt.ylabel('F1 Score', fontsize=24)\n",
    "plt.ylim([0, 1.02])\n",
    "plt.grid()\n",
    "#plt.savefig('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(list(range(1, len(best_f1_score)+1)), best_f1_score, '-', LineWidth=3, label=\"Best F1 Score\")\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.xlabel('Numer of Iterations', fontsize=24)\n",
    "plt.title(data_name+\"_\"+rnn, fontsize=24)\n",
    "plt.ylabel('F1 Score', fontsize=24)\n",
    "plt.ylim([0, 1.02])\n",
    "plt.grid()\n",
    "#plt.savefig('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(best_f1_score, 0.0)/len(best_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
