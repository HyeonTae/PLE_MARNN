{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameter\n",
    "- Attention = True\n",
    "- Teacher Forcing Ratio = 0.5\n",
    "- Dropout = 0.2\n",
    "- Layer = 4\n",
    "- Batch size = 128\n",
    "- Learning rate = 0.001\n",
    "- Hidden unit = 300\n",
    "- Epochs = 20\n",
    "- Data = ids\n",
    "- Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "\n",
    "import useful packages for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import sqlite3\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchtext\n",
    "\n",
    "os.chdir(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))))\n",
    "\n",
    "from models.seq2seq import Seq2seq\n",
    "from loss.loss import Perplexity\n",
    "from optim.optim import Optimizer\n",
    "from dataset import fields\n",
    "from evaluator.predictor import Predictor\n",
    "from util.helpers import apply_fix, vstack_with_right_padding, make_dir_if_not_exists\n",
    "from util.helpers import InvalidFixLocationException, SubstitutionFailedException\n",
    "from util.helpers import get_lines, extract_line_number, FailedToGetLineNumberException, _truncate_fix\n",
    "from util.helpers import tokens_to_source, compilation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FORMAT = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    "logging.basicConfig(format=LOG_FORMAT, level=getattr(logging, \"info\".upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = \"lstm\"\n",
    "data_name = \"ids\"\n",
    "data_type = \"raw\"\n",
    "pretrained_dir_name = None\n",
    "select = \"1\"\n",
    "batch_size = 128\n",
    "iteration = 5\n",
    "\n",
    "train_path = \"data/network_inputs/iitk-\"+data_name+\"-1189\"+\"/data_train_edit.txt\"\n",
    "test_path = \"data/network_inputs/iitk-\"+data_name+\"-1189\"\n",
    "config_path = \"models/config.json\"\n",
    "inverse_vocab_path = \"data/data_generator/target_vocab_reverse.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 450\n",
    "src = fields.SourceField()\n",
    "tgt = fields.TargetField()\n",
    "def len_filter(example):\n",
    "    return len(example.src) <= max_len and len(example.tgt) <= max_len\n",
    "train = torchtext.data.TabularDataset(\n",
    "    path=train_path, format='tsv',\n",
    "    fields=[('src', src), ('tgt', tgt)],\n",
    "    filter_pred=len_filter)\n",
    "src.build_vocab(train)\n",
    "tgt.build_vocab(train)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeontae/hyeontae/venv1/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "weight = torch.ones(len(tgt.vocab))\n",
    "pad = tgt.vocab.stoi[tgt.pad_token]\n",
    "loss = Perplexity(weight, pad)\n",
    "if torch.cuda.is_available():\n",
    "    loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"max_len\": 450,\n",
      "    \"embedding_size\": 50,\n",
      "    \"hidden_size\": 300,\n",
      "    \"input_dropout_p\": 0,\n",
      "    \"dropout_p\": 0.2,\n",
      "    \"n_layers\": 4,\n",
      "    \"bidirectional\": false,\n",
      "    \"rnn_cell\": \"lstm\",\n",
      "    \"variable_lengths\": true,\n",
      "    \"embedding\": null,\n",
      "    \"update_embedding\": true,\n",
      "    \"get_context_vector\": false,\n",
      "    \"use_attention\": true,\n",
      "    \"attn_layers\": 1,\n",
      "    \"hard_attn\": false,\n",
      "    \"position_embedding\": \"length\",\n",
      "    \"pos_add\": \"add\",\n",
      "    \"use_memory\": null,\n",
      "    \"memory_dim\": 5\n",
      "}\n",
      "Save_path : ids_att_with_pos_length_emb50_hidden300\n"
     ]
    }
   ],
   "source": [
    "optimizer = \"Adam\"\n",
    "seq2seq = None\n",
    "config_json = open(config_path).read()\n",
    "config = json.loads(config_json)\n",
    "config[\"max_len\"] = 450\n",
    "config[\"hidden_size\"] = 300\n",
    "config[\"rnn_cell\"] = rnn\n",
    "config[\"n_layers\"] = 4\n",
    "config[\"dropout_p\"] = 0.2\n",
    "config[\"embedding_size\"] = 50\n",
    "config[\"use_attention\"] = True\n",
    "config[\"position_embedding\"] = \"length\"\n",
    "config[\"use_memory\"] = None\n",
    "#config[\"seed\"]= 1189\n",
    "#config[\"pos_add\"] = \"cat\"\n",
    "\n",
    "print(json.dumps(config, indent=4))\n",
    "    \n",
    "save_path = (data_name\n",
    "            + (\"_att\" if config[\"use_attention\"] else \"\")\n",
    "            + (\"_with_pos_\" + config[\"position_embedding\"] if config[\"position_embedding\"] is not None else \"\")\n",
    "            + (\"_cat\" if config[\"pos_add\"] == \"cat\" else \"\")\n",
    "            + (\"_use_stack\" if config[\"use_memory\"] == \"stack\" else \"\")\n",
    "            + (\"_use_queue\" if config[\"use_memory\"] == \"queue\" else \"\")\n",
    "            + \"_emb\" + str(config[\"embedding_size\"])\n",
    "            + \"_hidden\" + str(config[\"hidden_size\"])\n",
    "            + (\"_pretrained\" if pretrained_dir_name is not None else \"\"))\n",
    "print(\"Save_path : %s\" % save_path)\n",
    "        \n",
    "if pretrained_dir_name is not None:\n",
    "    pretrained_path = (\"pretrained_weights/\"+ data_name + \"_\" + pretrained_dir_name\n",
    "            + (\"_att\" if config[\"use_attention\"] else \"\")\n",
    "            + (\"_with_pos_\" + config[\"position_embedding\"] if config[\"position_embedding\"] is not None else \"\")\n",
    "            + (\"_cat\" if config[\"pos_add\"] == \"cat\" else \"\")\n",
    "            + (\"_use_stack\" if config[\"use_memory\"] == \"stack\" else \"\")\n",
    "            + (\"_use_queue\" if config[\"use_memory\"] == \"queue\" else \"\")\n",
    "            + \"_emb\" + str(config[\"embedding_size\"])\n",
    "            + \"_hidden\" + str(config[\"hidden_size\"])\n",
    "            + \"_\"+rnn+\"_\"+str(i))\n",
    "    pretrained_pos_weight = np.load(pretrained_path+\"/encoder_pos_weight.npy\")\n",
    "    seq2seq = Seq2seq(config, vocab_size, vocab_size, sos_id, eos_id,\n",
    "                      pretrained_pos_weight)\n",
    "else :\n",
    "    seq2seq = Seq2seq(config, len(src.vocab), len(tgt.vocab), tgt.sos_id, tgt.eos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path = \"log/test/\" + save_path\n",
    "if not os.path.isdir(fig_path):\n",
    "    os.mkdir(fig_path)\n",
    "fig_path = fig_path + \"/\" + rnn\n",
    "if not os.path.isdir(fig_path):\n",
    "    os.mkdir(fig_path)\n",
    "database_path = fig_path + \"/\" + data_type\n",
    "if not os.path.isdir(database_path):\n",
    "    os.mkdir(database_path)\n",
    "database = database_path + \"/\" + data_name + \"_\" + data_type + \".db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    seq2seq.cuda()\n",
    "\n",
    "for param in seq2seq.parameters():\n",
    "    param.data.uniform_(-0.08, 0.08)\n",
    "            \n",
    "log_path = \"log/pth/\"+save_path +\"_\" + rnn + \"_\" + str(select) + \"_model_save.pth\"\n",
    "seq2seq.load_state_dict(torch.load(log_path))\n",
    "seq2seq.eval()\n",
    "\n",
    "predictor = Predictor(seq2seq, input_vocab, output_vocab, output_vocab.stoi[train.fields['tgt'].pad_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fix(program):\n",
    "    tgt_seq = predictor.predict_batch(program)\n",
    "    return tgt_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_raw data length : 6978\n"
     ]
    }
   ],
   "source": [
    "if data_type == 'raw':\n",
    "    test_dataset = np.load(os.path.join(\n",
    "        test_path, 'test_%s.npy' % (data_type))).item()\n",
    "else:\n",
    "    test_dataset = np.load(os.path.join(\n",
    "        test_path, 'test_%s-%s.npy' % (data_type, data_name))).item()\n",
    "    \n",
    "print(\"test_{} data length : {}\".format(data_type, sum([len(test_dataset[pid]) for pid in test_dataset])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f18b1e74730>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect(database)\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS programs (\n",
    "                prog_id text NOT NULL,\n",
    "                user_id text NOT NULL,\n",
    "                prob_id text NOT NULL,\n",
    "                code text NOT NULL,\n",
    "                name_dict text NOT NULL,\n",
    "                name_seq text NOT NULL,\n",
    "                PRIMARY KEY(prog_id)\n",
    "             )''')\n",
    "\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS iterations (\n",
    "                prog_id text NOT NULL,\n",
    "                iteration text NOT NULL,\n",
    "                network text NOT NULL,\n",
    "                fix text NOT NULL,\n",
    "                PRIMARY KEY(prog_id, iteration)\n",
    "             )''')\n",
    "\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS error_messages (\n",
    "                prog_id text NOT NULL,\n",
    "                iteration text NOT NULL,\n",
    "                network text NOT NULL,\n",
    "                error_message text NOT NULL,\n",
    "                FOREIGN KEY(prog_id, iteration, network) REFERENCES iterations(prog_id, iteration, network)\n",
    "             )''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_of_programs = {}\n",
    "fixes_suggested_by_network = {}\n",
    "\n",
    "if data_name == 'typo':\n",
    "    normalize_names = True\n",
    "    fix_kind = 'replace'\n",
    "else:\n",
    "    assert data_name == 'ids'\n",
    "    normalize_names = False\n",
    "    fix_kind = 'insert'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove line numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_line_numbers(source):\n",
    "    lines = source.count('~')\n",
    "    for l in range(lines):\n",
    "        if l >= 10:\n",
    "            source = source.replace(list(str(l))[0] + \" \" + list(str(l))[1] + \" ~ \", \"\", 1)\n",
    "        else:\n",
    "            source = source.replace(str(l) + \" ~ \", \"\", 1)\n",
    "    source = source.replace(\"  \", \" \")\n",
    "    return source.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(inverse_vocab_path, \"r\") as json_file:\n",
    "    inverse_vocab = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_edits(source, edits):\n",
    "    fixed = []\n",
    "    inserted = 0\n",
    "    insert_tok = [str(i) for i in range(1,110)]\n",
    "    for i, edit in enumerate(edits):\n",
    "        if i - inserted >= len(source):\n",
    "            break\n",
    "        if edit == '0':\n",
    "            fixed.append(source[i - inserted])\n",
    "        elif edit != '-1':\n",
    "            fixed.append(inverse_vocab[str(int(edit)+1)])\n",
    "            if edits[inserted] not in insert_tok:\n",
    "                inserted += 1\n",
    "    return fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor problem_id, test_programs in tqdm(test_dataset.items()):\\n    sequences_of_programs[problem_id] = {}\\n    fixes_suggested_by_network[problem_id] = {}\\n\\n    entries = []\\n    \\n    for program, name_dict, name_sequence, user_id, program_id in test_programs:\\n        sequences_of_programs[problem_id][program_id] = [program]\\n        fixes_suggested_by_network[problem_id][program_id] = []\\n        entries.append(\\n            (program, name_dict, name_sequence, user_id, program_id,))\\n\\n        c.execute(\"INSERT OR IGNORE INTO programs VALUES (?,?,?,?,?,?)\", (program_id,\\n                  user_id, problem_id, program, json.dumps(name_dict), json.dumps(name_sequence)))\\n            \\n\\n    input_ = []\\n        \\n    for i, entry in enumerate(entries):\\n        _, _, _, _, program_id = entry\\n\\n        tmp = sequences_of_programs[problem_id][program_id][-1]\\n        input_.append(remove_line_numbers(tmp))\\n\\n        cnt = 0\\n        fixes = []\\n        for i in range(math.ceil(len(input_)/batch_size)):\\n            if cnt+batch_size > len(input_):\\n                fix = get_fix(input_[cnt:len(input_)])\\n            else:\\n                fix = get_fix(input_[cnt:cnt+batch_size])\\n            cnt += batch_size\\n            fixes += fix\\n    \\n\\n        for source, fix in zip(input_, fixes):\\n            program = apply_edits(source, fix.split())\\n            c.execute(\"INSERT OR IGNORE INTO iterations VALUES (?,?,?,?)\",\\n                     (program_id, 1, data_name, fix))\\n    conn.commit()\\nconn.commit()\\nconn.close()\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for problem_id, test_programs in tqdm(test_dataset.items()):\n",
    "    sequences_of_programs[problem_id] = {}\n",
    "    fixes_suggested_by_network[problem_id] = {}\n",
    "\n",
    "    entries = []\n",
    "    \n",
    "    for program, name_dict, name_sequence, user_id, program_id in test_programs:\n",
    "        sequences_of_programs[problem_id][program_id] = [program]\n",
    "        fixes_suggested_by_network[problem_id][program_id] = []\n",
    "        entries.append(\n",
    "            (program, name_dict, name_sequence, user_id, program_id,))\n",
    "\n",
    "        c.execute(\"INSERT OR IGNORE INTO programs VALUES (?,?,?,?,?,?)\", (program_id,\n",
    "                  user_id, problem_id, program, json.dumps(name_dict), json.dumps(name_sequence)))\n",
    "            \n",
    "\n",
    "    input_ = []\n",
    "        \n",
    "    for i, entry in enumerate(entries):\n",
    "        _, _, _, _, program_id = entry\n",
    "\n",
    "        tmp = sequences_of_programs[problem_id][program_id][-1]\n",
    "        input_.append(remove_line_numbers(tmp))\n",
    "\n",
    "        cnt = 0\n",
    "        fixes = []\n",
    "        for i in range(math.ceil(len(input_)/batch_size)):\n",
    "            if cnt+batch_size > len(input_):\n",
    "                fix = get_fix(input_[cnt:len(input_)])\n",
    "            else:\n",
    "                fix = get_fix(input_[cnt:cnt+batch_size])\n",
    "            cnt += batch_size\n",
    "            fixes += fix\n",
    "    \n",
    "\n",
    "        for source, fix in zip(input_, fixes):\n",
    "            program = apply_edits(source, fix.split())\n",
    "            c.execute(\"INSERT OR IGNORE INTO iterations VALUES (?,?,?,?)\",\n",
    "                     (program_id, 1, data_name, fix))\n",
    "    conn.commit()\n",
    "conn.commit()\n",
    "conn.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/93 [00:00<?, ?it/s]/home/hyeontae/hyeontae/venv1/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "100%|██████████| 93/93 [02:58<00:00,  1.83s/it]\n"
     ]
    }
   ],
   "source": [
    "for problem_id, test_programs in tqdm(test_dataset.items()):\n",
    "    sequences_of_programs[problem_id] = {}\n",
    "    fixes_suggested_by_network[problem_id] = {}\n",
    "\n",
    "    entries = []\n",
    "    \n",
    "    for program, name_dict, name_sequence, user_id, program_id in test_programs:\n",
    "        sequences_of_programs[problem_id][program_id] = [program]\n",
    "        fixes_suggested_by_network[problem_id][program_id] = []\n",
    "        entries.append(\n",
    "            (program, name_dict, name_sequence, user_id, program_id,))\n",
    "\n",
    "        c.execute(\"INSERT OR IGNORE INTO programs VALUES (?,?,?,?,?,?)\", (program_id,\n",
    "                  user_id, problem_id, program, json.dumps(name_dict), json.dumps(name_sequence)))\n",
    "            \n",
    "    for round_ in range(iteration):\n",
    "        to_delete = []\n",
    "        input_ = []\n",
    "    \n",
    "        for i, entry in enumerate(entries):\n",
    "            _, _, _, _, program_id = entry\n",
    "\n",
    "            if sequences_of_programs[problem_id][program_id][-1] is not None:\n",
    "                tmp = sequences_of_programs[problem_id][program_id][-1]\n",
    "                input_.append(remove_line_numbers(tmp))\n",
    "            else:\n",
    "                to_delete.append(i)\n",
    "                \n",
    "        to_delete = sorted(to_delete)[::-1]\n",
    "\n",
    "        for i in to_delete:\n",
    "            del entries[i]\n",
    "\n",
    "        assert len(input_) == len(entries)\n",
    "\n",
    "        if len(input_) == 0:\n",
    "            print('Stopping before iteration %d (no programs remain)' % (round_ + 1))\n",
    "            break\n",
    "            \n",
    "        cnt = 0\n",
    "        fixes = []\n",
    "        for i in range(math.ceil(len(input_)/batch_size)):\n",
    "            if cnt+batch_size > len(input_):\n",
    "                fix = get_fix(input_[cnt:len(input_)])\n",
    "            else:\n",
    "                fix = get_fix(input_[cnt:cnt+batch_size])\n",
    "            cnt += batch_size\n",
    "            fixes += fix\n",
    "            \n",
    "        to_delete = []\n",
    "\n",
    "        # Apply fixes\n",
    "        for i, entry, fix in zip(range(len(fixes)), entries, fixes):\n",
    "            _, _, _, _, program_id = entry\n",
    "            if sum(list(map(int, fix.split()))) == 0:\n",
    "                to_delete.append(i)\n",
    "            else:\n",
    "                program = apply_edits(remove_line_numbers(sequences_of_programs[problem_id][program_id][-1])\n",
    "                                      , fix.split())\n",
    "                sequences_of_programs[problem_id][program_id].append(\" \".join(program))\n",
    "                \n",
    "            c.execute(\"INSERT OR IGNORE INTO iterations VALUES (?,?,?,?)\",\n",
    "                     (program_id, round_ + 1, data_name, fix))\n",
    "                  \n",
    "        to_delete = sorted(to_delete)[::-1]\n",
    "        \n",
    "        for i in to_delete:\n",
    "            del entries[i]\n",
    "            \n",
    "    conn.commit()\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_results(database):\n",
    "    with sqlite3.connect(database) as conn:\n",
    "        c = conn.cursor()\n",
    "\n",
    "        error_counts = []\n",
    "\n",
    "        for row in c.execute(\"SELECT iteration, COUNT(*) FROM error_messages GROUP BY iteration ORDER BY iteration;\"):\n",
    "            error_counts.append(row[1])\n",
    "\n",
    "        query1 = \"\"\"SELECT COUNT(*)\n",
    "        FROM error_messages\n",
    "        WHERE iteration = 0 AND prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"\"\"\n",
    "\n",
    "        for row in c.execute(query1):\n",
    "            initial_errors = row[0]\n",
    "\n",
    "        query2 = \"\"\"SELECT COUNT(*)\n",
    "        FROM error_messages\n",
    "        WHERE iteration = 10 AND prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"\"\"\n",
    "\n",
    "        for row in c.execute(query2):\n",
    "            final_errors = row[0]\n",
    "\n",
    "        query3 = \"\"\"SELECT COUNT(DISTINCT prog_id)\n",
    "        FROM error_message_strings\n",
    "        WHERE iteration = 10 AND error_message_count = 0 and prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"\"\"\n",
    "\n",
    "        for row in c.execute(query3):\n",
    "            fully_fixed = row[0]\n",
    "\n",
    "        query4 = \"\"\"SELECT DISTINCT prog_id, error_message_count FROM error_message_strings\n",
    "        WHERE iteration = 0 AND error_message_count > 0 and prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"\"\"\n",
    "\n",
    "        query5 = \"\"\"SELECT DISTINCT prog_id, error_message_count FROM error_message_strings\n",
    "        WHERE iteration = 10 AND error_message_count > 0 and prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"\"\"\n",
    "\n",
    "        original_errors = {}\n",
    "        for row in c.execute(query4):\n",
    "            original_errors[row[0]] = int(row[1])\n",
    "\n",
    "        partially_fixed = {}\n",
    "        unfixed = {}\n",
    "        for row in c.execute(query5):\n",
    "            if int(row[1]) < original_errors[row[0]]:\n",
    "                partially_fixed[row[0]] = int(row[1])\n",
    "            elif int(row[1]) == original_errors[row[0]]:\n",
    "                unfixed[row[0]] = int(row[1])\n",
    "            else:\n",
    "                print(row[0], row[1], original_errors[row[0]])\n",
    "\n",
    "        token_counts = []\n",
    "        assignments = None\n",
    "\n",
    "        for row in c.execute(\"SELECT COUNT(DISTINCT prob_id) FROM programs p WHERE prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"):\n",
    "            assignments = int(row[0])\n",
    "\n",
    "        for row in c.execute(\"SELECT code FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count <> 0;\"):\n",
    "            token_counts += [len(row[0].split())]\n",
    "\n",
    "        avg_token_count = np.mean(token_counts)\n",
    "\n",
    "        print(\"-------\")\n",
    "        print(\"Assignments:\", assignments)\n",
    "        print(\"Program count:\", len(token_counts))\n",
    "        print(\"Average token count:\", avg_token_count)\n",
    "        print(\"Error messages:\", initial_errors)\n",
    "        print(\"-------\")\n",
    "\n",
    "        print(\"Errors remaining:\", final_errors)\n",
    "        print(\"Reduction in errors:\", (initial_errors - final_errors))\n",
    "        print(\"Completely fixed programs:\", fully_fixed)\n",
    "        print(\"partially fixed programs:\", len(partially_fixed))\n",
    "        print(\"unfixed programs:\", len(unfixed))\n",
    "        print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_problem(problem_id):\n",
    "    global reconstruction, errors, errors_full, total_count, errors_test\n",
    "\n",
    "    c = conn.cursor()\n",
    "\n",
    "    reconstruction[problem_id] = {}\n",
    "    errors[problem_id] = {}\n",
    "    errors_full[problem_id] = {}\n",
    "    errors_test[problem_id] = []\n",
    "    candidate_programs = []\n",
    "\n",
    "    for row in c.execute('SELECT user_id, prog_id, code, name_dict, name_seq FROM programs WHERE prob_id = ?', (problem_id,)):\n",
    "        user_id, prog_id, initial = row[0], row[1], \" \".join(remove_line_numbers(row[2]))\n",
    "        name_dict = json.loads(row[3])\n",
    "        name_seq = json.loads(row[4])\n",
    "\n",
    "        candidate_programs.append(\n",
    "            (user_id, prog_id, initial, name_dict, name_seq,))\n",
    "\n",
    "    for _, prog_id, initial, name_dict, name_seq in candidate_programs:\n",
    "        fixes_suggested_by_typo_network = []\n",
    "        fixes_suggested_by_undeclared_network = []\n",
    "\n",
    "        for row in c.execute('SELECT fix FROM iterations WHERE prog_id=? AND network = \\'typo\\' ORDER BY iteration', (prog_id,)):\n",
    "            fixes_suggested_by_typo_network.append(row[0])\n",
    "\n",
    "        for row in c.execute('SELECT fix FROM iterations WHERE prog_id=? AND network = \\'ids\\' ORDER BY iteration', (prog_id,)):\n",
    "            fixes_suggested_by_undeclared_network.append(row[0])\n",
    "\n",
    "        reconstruction[problem_id][prog_id] = [initial]\n",
    "        temp_errors, temp_errors_full = compilation_errors(\n",
    "            tokens_to_source(initial, name_dict, False), database_path)\n",
    "        errors[problem_id][prog_id] = [temp_errors]\n",
    "        errors_full[problem_id][prog_id] = [temp_errors_full]\n",
    "\n",
    "        for fix in fixes_suggested_by_typo_network:\n",
    "            temp_prog = \" \".join(apply_edits(\n",
    "                reconstruction[problem_id][prog_id][-1] , fix.split()))\n",
    "            temp_errors, temp_errors_full = compilation_errors(\n",
    "                tokens_to_source(temp_prog, name_dict, False), database_path)\n",
    "\n",
    "            if len(temp_errors) > len(errors[problem_id][prog_id][-1]):\n",
    "                break\n",
    "            else:\n",
    "                reconstruction[problem_id][prog_id].append(temp_prog)\n",
    "                errors[problem_id][prog_id].append(temp_errors)\n",
    "                errors_full[problem_id][prog_id].append(\n",
    "                    temp_errors_full)\n",
    "\n",
    "        while len(reconstruction[problem_id][prog_id]) <= 5:\n",
    "            reconstruction[problem_id][prog_id].append(\n",
    "                reconstruction[problem_id][prog_id][-1])\n",
    "            errors[problem_id][prog_id].append(errors[problem_id][prog_id][-1])\n",
    "            errors_full[problem_id][prog_id].append(\n",
    "                errors_full[problem_id][prog_id][-1])\n",
    "\n",
    "        already_fixed = []\n",
    "        for fix in fixes_suggested_by_undeclared_network:\n",
    "            if fix not in already_fixed:\n",
    "                temp_prog = \" \".join(apply_edits(\n",
    "                    reconstruction[problem_id][prog_id][-1] , fix.split()))\n",
    "                already_fixed.append(fix)\n",
    "                temp_errors, temp_errors_full = compilation_errors(\n",
    "                    tokens_to_source(temp_prog, name_dict, False), database_path)\n",
    "\n",
    "                if len(temp_errors) > len(errors[problem_id][prog_id][-1]):\n",
    "                    break\n",
    "                else:\n",
    "                    reconstruction[problem_id][prog_id].append(temp_prog)\n",
    "                    errors[problem_id][prog_id].append(temp_errors)\n",
    "                    errors_full[problem_id][prog_id].append(\n",
    "                        temp_errors_full)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        while len(reconstruction[problem_id][prog_id]) <= 10:\n",
    "            reconstruction[problem_id][prog_id].append(\n",
    "                reconstruction[problem_id][prog_id][-1])\n",
    "            errors[problem_id][prog_id].append(errors[problem_id][prog_id][-1])\n",
    "            errors_full[problem_id][prog_id].append(\n",
    "                errors_full[problem_id][prog_id][-1])\n",
    "\n",
    "        errors_test[problem_id].append(errors[problem_id][prog_id])\n",
    "\n",
    "        for k, errors_t, errors_full_t in zip(range(len(errors[problem_id][prog_id])), errors[problem_id][prog_id], errors_full[problem_id][prog_id]):\n",
    "            c.execute(\"INSERT INTO error_message_strings VALUES(?, ?, ?, ?, ?)\", (\n",
    "                prog_id, k, 'typo', errors_full_t.decode('utf-8', 'ignore'), len(errors_t)))\n",
    "\n",
    "            for error_ in errors_t:\n",
    "                c.execute(\"INSERT INTO error_messages VALUES(?, ?, ?, ?)\",\n",
    "                            (prog_id, k, 'typo', error_.decode('utf-8', 'ignore'),))\n",
    "\n",
    "    count_t = len(candidate_programs)\n",
    "    total_count += count_t\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(database)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS error_message_strings (\n",
    "                prog_id text NOT NULL,\n",
    "                iteration text NOT NULL,\n",
    "                network text NOT NULL,\n",
    "                error_message_string text NOT NULL,\n",
    "                error_message_count integer NOT NULL,\n",
    "                FOREIGN KEY(prog_id, iteration, network) REFERENCES iterations(prog_id, iteration, network)\n",
    "             )''')\n",
    "\n",
    "problem_ids = []\n",
    "\n",
    "for row in c.execute('SELECT DISTINCT prob_id FROM programs'):\n",
    "    problem_ids.append(row[0])\n",
    "\n",
    "c.close()\n",
    "\n",
    "reconstruction = {}\n",
    "errors = {}\n",
    "errors_full = {}\n",
    "errors_test = {}\n",
    "\n",
    "fixes_per_stage = [0] * 10\n",
    "\n",
    "total_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [42:35<00:00, 26.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 2555.713000535965 seconds\n",
      "Total programs processed: 6978\n",
      "Average time per program: 366 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for problem_id in tqdm(problem_ids):\n",
    "    do_problem(problem_id)\n",
    "\n",
    "time_t = time.time() - start\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print('Total time:', time_t, 'seconds')\n",
    "print('Total programs processed:', total_count)\n",
    "print('Average time per program:', int(float(time_t) / float(total_count) * 1000), 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 %\n",
      "Stage 0 : 0\n",
      "Stage 1 : 0\n",
      "Stage 2 : 0\n",
      "Stage 3 : 0\n",
      "Stage 4 : 0\n",
      "Stage 5 : 21\n",
      "Stage 6 : 1\n",
      "Stage 7 : 0\n",
      "Stage 8 : 1\n",
      "Stage 9 : 0\n",
      "-------\n",
      "Assignments: 93\n",
      "Program count: 6978\n",
      "Average token count: 203.26526225279449\n",
      "Error messages: 7001\n",
      "-------\n",
      "Errors remaining: 6978\n",
      "Reduction in errors: 23\n",
      "Completely fixed programs: 0\n",
      "partially fixed programs: 6\n",
      "unfixed programs: 6972\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "total_fixes_num = {}\n",
    "errors_before = {}\n",
    "\n",
    "for problem_id in errors_test:\n",
    "    total_fixes_num[problem_id] = 0\n",
    "\n",
    "    for j, seq in enumerate(errors_test[problem_id]):\n",
    "        error_numbers = [len(x) for x in seq]\n",
    "        skip = False\n",
    "\n",
    "        for i in range(len(error_numbers) - 1):\n",
    "            assert (not error_numbers[i + 1] > error_numbers[i])\n",
    "            total_fixes_num[problem_id] += error_numbers[i] - \\\n",
    "                error_numbers[i + 1]\n",
    "\n",
    "            if error_numbers[i] != error_numbers[i + 1]:\n",
    "                fixes_per_stage[i] += error_numbers[i] - error_numbers[i + 1]\n",
    "\n",
    "total_numerator = 0\n",
    "total_denominator = 0\n",
    "\n",
    "for problem_id in errors_test:\n",
    "    total_numerator += total_fixes_num[problem_id]\n",
    "    total_denominator += sum([len(x[0]) for x in errors_test[problem_id]])\n",
    "\n",
    "\n",
    "print(int(float(total_numerator) * 100.0 / float(total_denominator)), '%')\n",
    "\n",
    "\n",
    "for stage in range(len(fixes_per_stage)):\n",
    "    print('Stage', stage, ':', fixes_per_stage[stage])\n",
    "\n",
    "get_final_results(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
