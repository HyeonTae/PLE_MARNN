#!/usr/bin/env python
# coding: utf-8

# # Set Parameter
# - Attention = True
# - Teacher Forcing Ratio = 0.5
# - Layer = 1
# - Batch size = 32
# - Learning rate = 0.001
# - Hidden unit = 200
# - Epochs = 60
# - N = 100
# - Data Length = 100K
# - Data = single_Ctype4_error_rate_1
# - Deduplication
# - Random split

# # Import packages
# import useful packages for experiments
import os
import argparse
import logging
import sys
import json

import torch
from torch.optim.lr_scheduler import StepLR
import torchtext

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))))))
os.chdir(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))))))

from models.trainer import Trainer
from models.seq2seq import Seq2seq
from loss.loss import Perplexity
from dataset.load_data import LoadData

import matplotlib.pyplot as plt


# # Log format
log_level = 'info'
LOG_FORMAT = '%(asctime)s %(levelname)-6s %(message)s'
logging.basicConfig(format=LOG_FORMAT, level=getattr(logging, log_level.upper()))

character_accuracy = []
sentence_accuracy = []
f1_score = []
best_f1_score = []
rnn = "lstm"
data_name = "typo"
pretrained_dir_name = None
iterator = list(range(1,11,1))
epochs = 100

data_path = "data/network_inputs/iitk-"+data_name+"-1189/bin_"
config_path = "models/config.json"

# # Prepare dataset
for i in iterator:
    for bins in range(5):
        dataset = LoadData(data_path + str(bins))
        vocab = tl_dict = dataset.get_tl_dictionary()
        vocab_size = dataset.vocabulary_size
        sos_id = 1
        eos_id = 1
        pad = 0

        weight = torch.ones(vocab_size)
        loss = Perplexity(weight, pad)
        if torch.cuda.is_available():
            loss.cuda()

        optimizer = "Adam"
        seq2seq = None
        config_json = open(config_path).read()
        config = json.loads(config_json)
        config["hidden_size"] = 100
        config["rnn_cell"] = rnn
        config["embedding_size"] = 50
        config["use_attention"] = True
        config["encoder_position_embedding"] = "length"
        config["decoder_position_embedding"] = "length"
        config["use_memory"] = None
        #config["pos_add"] = "cat"

        print(json.dumps(config, indent=4))

        save_path = (data_name + "_bin_" + str(bins)
                        + ("_att" if config["use_attention"] else "")
                        + ("_with_pos" if config["encoder_position_embedding"] is not None 
                           or config["decoder_position_embedding"] is not None else "")
                        + ("_encoder_" + config["encoder_position_embedding"]
                           if config["encoder_position_embedding"] is not None else "")
                        + ("_decoder_" + config["decoder_position_embedding"]
                           if config["decoder_position_embedding"] is not None else "")
                        + ("_cat" if config["pos_add"] == "cat" else "")
                        + ("_use_stack" if config["use_memory"] == "stack" else "")
                        + ("_use_queue" if config["use_memory"] == "queue" else "")
                        + "_emb" + str(config["embedding_size"])
                        + "_hidden" + str(config["hidden_size"])
                        + ("_pretrained" if pretrained_dir_name is not None else ""))
        print("Save_path : %s" % save_path)
        
        if pretrained_dir_name is not None:
            pretrained_path = ("pretrained_weights/"+ data_name + "_bin_" + str(bins) + "_" + pretrained_dir_name
                        + ("_att" if config["use_attention"] else "")
                        + ("_with_pos" if config["encoder_position_embedding"] is not None 
                           or config["decoder_position_embedding"] is not None else "")
                        + ("_encoder_" + config["encoder_position_embedding"]
                           if config["encoder_position_embedding"] is not None else "")
                        + ("_decoder_" + config["decoder_position_embedding"]
                           if config["decoder_position_embedding"] is not None else "")
                        + ("_cat" if config["pos_add"] == "cat" else "")
                        + ("_use_stack" if config["use_memory"] == "stack" else "")
                        + ("_use_queue" if config["use_memory"] == "queue" else "")
                        + "_emb" + str(config["embedding_size"])
                        + "_hidden" + str(config["hidden_size"])
                        + "_"+rnn+"_"+str(i))
            encoder_pos_weight = np.load(pretrained_path+"/encoder_pos_weight.npy")
            decoder_pos_weight = np.load(pretrained_path+"/decoder_pos_weight.npy")
            seq2seq = Seq2seq(config, vocab_size, vocab_size, sos_id, eos_id,
                              encoder_pos_weight, decoder_pos_weight)
        else :
            seq2seq = Seq2seq(config, vocab_size, vocab_size, sos_id, eos_id)
        
        if torch.cuda.is_available():
            seq2seq.cuda()

        for param in seq2seq.parameters():
            param.data.uniform_(-0.08, 0.08)

        # train
        t = Trainer(loss=loss, batch_size=128,
                    learning_rate=0.002,
                    checkpoint_every=50,
                    print_every=100,
                    hidden_size=config["hidden_size"],
                    path=save_path,
                    file_name=config["rnn_cell"] + "_" + str(i))

        seq2seq, ave_loss, character_accuracy_list, sentence_accuracy_list, f1_score_list = t.train(seq2seq, dataset,
                                                                                 num_epochs=epochs,
                                                                                 optimizer=optimizer,
                                                                                 teacher_forcing_ratio=0.5)

        character_accuracy.append(character_accuracy_list)
        sentence_accuracy.append(sentence_accuracy_list)
        f1_score.append(f1_score_list)
        best_f1_score.append(max(f1_score_list))

# svae plot
#------- Sentence Accuracy -------
plt.figure(figsize=(15,7))
for j in range(len(sentence_accuracy)):
    plt.plot(list(range(1, len(sentence_accuracy[j])+1, 1))[::3], sentence_accuracy[j][::3], '-', LineWidth=3, label=str(j+1))

plt.legend(loc="best", fontsize=12)
plt.xlabel('Epoch', fontsize=24)
plt.ylabel('Sentence Accuracy', fontsize=24)
plt.ylim([0, 1.02])
plt.grid()
plt.savefig("log/plot/"+save_path+"/"+rnn+"_sentence_accuracy.png")

#------- F1 Score -------
plt.figure(figsize=(15,7))
for j in range(len(f1_score)):
    plt.plot(list(range(1, len(f1_score[j])+1, 1))[::3], f1_score[j][::3], '-', LineWidth=3, label=str(j+1))

plt.legend(loc="best", fontsize=12)
plt.xlabel('Epoch', fontsize=24)
plt.ylabel('F1 Score', fontsize=24)
plt.ylim([0, 1.02])
plt.grid()
plt.savefig("log/plot/"+save_path+"/"+rnn+"_f1_score.png")

#------- Best F1 Score -------
plt.figure(figsize=(15,7))
plt.plot(list(range(1,len(best_f1_score)+1,1)), best_f1_score, '-', LineWidth=3, label="Best F1 Score")

plt.legend(loc="best", fontsize=12)
plt.xlabel('Numer of Iterations', fontsize=24)
plt.ylabel('F1 Score', fontsize=24)
plt.ylim([0, 1.02])
plt.grid()
plt.savefig("log/plot/"+save_path+"/"+rnn+"_best_f1_score.png")

print("\n\nFinish!")
